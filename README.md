# Real-time Monocular SLAM and 3D Reconstruction

This project implements a real-time monocular Simultaneous Localization and Mapping (SLAM) system with 3D reconstruction capabilities. It utilizes a single camera to estimate its motion, perceive depth in the scene, and build a 3D map of the environment.

## Features

*   **Monocular Camera Input:** Processes video feed from a standard webcam or video file.
*   **MiDaS Depth Estimation (PyTorch Hub):** Employs pre-trained MiDaS v3.1 models (e.g., DPT_Hybrid, MiDaS_small) from PyTorch Hub (`intel-isl/MiDaS`) to estimate depth from monocular images.
*   **Monocular SLAM Frontend:** Implements a basic SLAM frontend (`SLAMFrontend`) using ORB features. It handles keyframe selection, initial map point triangulation with a scaled baseline, and pose estimation via local map tracking (using `cv2.solvePnP`).
*   **TSDF Dense Reconstruction:** Builds a dense 3D map of the environment using Open3D's `ScalableTSDFVolume`, guided by poses from the `SLAMFrontend`.
*   **Sparse Map Visualization:** The main application can visualize the sparse 3D map points generated by the SLAM frontend (typically colored green).
*   **Point Cloud & Mesh Generation:** Can extract a global point cloud or a 3D mesh from the TSDF volume.
*   **Map Saving/Loading:**
    *   Saves the reconstructed dense point cloud map to a `.ply` file.
    *   Loads a `.ply` point cloud file for visualization of the dense map (note: live TSDF reconstruction will restart if a map is loaded this way).
*   **Camera Calibration:** Includes scripts (`scripts/calibrate_camera.py` and `scripts/calibration_assistant.py`) to calibrate the camera using checkerboard images, saving parameters to a YAML file. The interactive `calibration_assistant.py` features live checkerboard detection, zone coverage guidance, and an auto-capture mode (toggle with 'a' key). Default internal corners for calibration set to 12x8.
*   **Modular Design:** Code is organized into modules for camera handling, depth estimation, SLAM, and reconstruction.

## Project Structure

```
.
├── data/                     # Directory for input/output data
│   ├── calibration_images/   # Sample images for camera calibration (user provides their own)
│   ├── camera_calibration.yaml # Output of the calibration script (example provided)
│   └── generated_map_tsdf.ply  # Example of a saved map (ignored by Git)
├── environment.yml           # Conda environment definition
├── models/                   # Directory for user-saved models or other non-PyTorch Hub models. MiDaS models from PyTorch Hub are cached by PyTorch (see Setup).
├── README.md                 # This file
├── scripts/                  # Python scripts to run parts of or the full application
│   ├── calibrate_camera.py   # Script for camera calibration (manual image placement)
│   ├── calibration_assistant.py # Interactive script for camera calibration image collection & processing
│   ├── download_midas_model.py # Script to pre-download MiDaS models
│   ├── run_pointcloud_generation.py # Main script for full SLAM & reconstruction
│   ├── run_vo.py             # Script to run only the old Visual Odometry (for comparison/testing)
│   ├── view_camera.py        # Script to test camera input
│   └── view_depth.py         # Script to test depth estimation
├── src/                      # Source code for the SLAM system modules
│   ├── camera/               # MonocularCamera class
│   ├── depth_estimation/     # MiDaSDepthEstimator class
│   ├── reconstruction/       # PointCloudMapper class (with TSDF)
│   ├── slam/                 # SLAMFrontend class
│   └── utils/                # CameraParams class and other utilities
├── tests/                    # Unit tests
│   ├── __init__.py
│   └── test_camera_params.py
└── .gitignore                # Specifies intentionally untracked files
```

## Setup Instructions

It is recommended to use Python 3.11 to set up your local development environment, as this version has good compatibility with all project dependencies, including `open3d` on macOS.

1.  **Prerequisites:**
    *   Ensure you have Python 3.11 installed. You can download it from [python.org](https://www.python.org/).
    *   Ensure you have Pip installed and updated (usually comes with Python).

2.  **Clone the Repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

3.  **Create a Virtual Environment (Recommended):**
    It's highly recommended to use a virtual environment to manage project dependencies.
    ```bash
    python3.11 -m venv .venv
    ```
    Activate the virtual environment:
    *   On Windows:
        ```bash
        .\.venv\Scripts\activate
        ```
    *   On macOS and Linux:
        ```bash
        source .venv/bin/activate
        ```

4.  **Install Dependencies:**
    Install all necessary Python dependencies using the `requirements.txt` file:
    ```bash
    pip install -r requirements.txt
    ```

5.  **Camera Connection:**
    *   Ensure you have a webcam connected to your system if you intend to use live camera input.
    *   The default camera ID used is `0`. If you have multiple cameras, you might need to change this ID in the scripts (e.g., in `MonocularCamera(0)` calls).

6.  **Pre-cache MiDaS Model Weights (PyTorch Hub):**
    The system uses MiDaS v3.1 models (e.g., DPT_Hybrid, MiDaS_small) via PyTorch Hub (`intel-isl/MiDaS`). PyTorch Hub automatically handles the download and caching of these models (typically in `~/.cache/torch/hub/` on Linux/macOS or `C:\Users\<username>\.cache\torch\hub\` on Windows).

    The main application scripts (`run_pointcloud_generation.py`, `view_depth.py`) will trigger this download on their first run if the selected model is not already cached. To pre-cache models, especially if you plan to work offline or want to ensure all desired models are available without delay during application startup, use the `scripts/download_midas_model.py` script.

    **Usage of `scripts/download_midas_model.py`:**
    ```bash
    # Ensure the default 'hybrid' (DPT_Hybrid) MiDaS model is cached
    python scripts/download_midas_model.py

    # Ensure the 'small' (MiDaS_small) model is cached
    python scripts/download_midas_model.py --model_type small
    ```
    (See script for more options like `--model_type all`).

## Running with Docker

This project includes a `Dockerfile` to build a containerized environment with all dependencies pre-installed. The Docker image uses **Python 3.11**. (See previous README version for detailed Docker setup and run commands if needed, as the core Docker instructions remain similar. Ensure to mount `./data` and potentially `~/.cache/torch/hub`.)

## Running the Application

### 1. Camera Calibration (Recommended First Step)

Accurate camera intrinsic parameters are crucial for SLAM. Follow the instructions in the "Setup Instructions" section of the previous README version or use the `scripts/calibrate_camera.py` or `scripts/calibration_assistant.py` scripts. Calibration results are saved to `data/camera_calibration.yaml`.

### 2. Running the Full SLAM and 3D Reconstruction Pipeline (`run_pointcloud_generation.py`)

This is the main application script that integrates `SLAMFrontend`, MiDaS depth estimation, and TSDF-based dense reconstruction.

*   **To run with live camera feed and start a new map:**
    ```bash
    python scripts/run_pointcloud_generation.py
    ```
*   **To load an existing point cloud map for dense visualization (live TSDF will restart, SLAM state is unaffected):**
    ```bash
    python scripts/run_pointcloud_generation.py --load_map data/your_map.ply
    ```
    (Replace `data/your_map.ply` with the actual path to your map file).

### Critical: Calibrating `DEPTH_OUTPUT_SCALE_FACTOR`

The `SLAMFrontend` establishes an internal, arbitrary but consistent scale for its camera poses and sparse map. The MiDaS depth estimation, however, produces relative depth. For a coherent 3D reconstruction where the dense map (from MiDaS) aligns with the SLAM system's scale, you **MUST** tune the `DEPTH_OUTPUT_SCALE_FACTOR` in `scripts/run_pointcloud_generation.py`.

**The Goal:** Align the scaled MiDaS depth with the SLAM system's internal scale.

**Process:** This is a visual and iterative tuning process:
1.  **Initial Guess:** You might start with a rough estimate (e.g., based on a known object distance vs. raw MiDaS output, or a default like 10.0).
2.  **Run the System:** Execute `scripts/run_pointcloud_generation.py`.
3.  **Observe Visually:** In the Open3D window, compare the dense point cloud (from TSDF integration of MiDaS depth) with the sparse map points from SLAM (typically green) and the camera's movement.
    *   If the dense cloud looks "too flat" or "too close" relative to camera motion, `DEPTH_OUTPUT_SCALE_FACTOR` might be too small. Increase it.
    *   If the dense cloud looks "too deep" or "stretched out," the factor might be too large. Decrease it.
4.  **Iterate:** Adjust the factor in the script and re-run until the dense reconstruction aligns well with the SLAM trajectory and sparse map.

**For detailed step-by-step guidance on this calibration, please refer to the extensive comments directly within the `scripts/run_pointcloud_generation.py` script, above the `DEPTH_OUTPUT_SCALE_FACTOR` definition.**

### 3. Running Other Utility Scripts

These scripts are useful for testing individual components:

*   **Test Camera Input:** `python scripts/view_camera.py`
*   **Test Depth Estimation:** `python scripts/view_depth.py`
*   **Test Old Visual Odometry (for comparison):** `python scripts/run_vo.py`

## Keyboard Controls (for `run_pointcloud_generation.py`)

When the Open3D visualizer window is active:

*   `q`: Quit the application.
*   `k`: Save the current dense map (TSDF-extracted point cloud) to `data/generated_map_tsdf.ply`.
*   `l`: Load a dense map from `data/generated_map_tsdf.ply` (for TSDF visualization; live TSDF volume will be reset).
*   `m`: Extract and view the 3D mesh from the current TSDF volume in a new window.

## Dependencies

Key Python libraries used:

*   **OpenCV (`cv2`):** For camera handling, image processing, feature detection/matching.
*   **Open3D (`open3d`):** For 3D data structures, TSDF integration, and 3D visualization.
*   **PyTorch (`torch`), TorchVision (`torchvision`), TIMM (`timm`):** For MiDaS depth estimation.
*   **NumPy, PyYAML, Pandas, tqdm.**

All required Python packages are listed in `requirements.txt`.

## Known Issues / Limitations

*   **Overall Map Scale:** The `SLAMFrontend` sets an initial arbitrary but consistent scale based on the baseline between the first two keyframes. The `DEPTH_OUTPUT_SCALE_FACTOR` tuning aligns MiDaS depth to this. The resulting overall map scale is therefore not truly metric but is internally consistent.
*   **SLAM System Foundation:** The current `SLAMFrontend` is a foundational implementation. It lacks advanced features like robust local map management beyond the initial set of points, bundle adjustment, relocalization after tracking loss, or comprehensive loop closure.
*   **Depth Estimation Quality:** MiDaS performance varies with model size and scene. Errors in depth estimation directly impact reconstruction quality.
*   **SLAM Robustness:** Feature-based SLAM can be sensitive to:
    *   Fast camera movements or rotations.
    *   Poorly textured environments.
    *   Significant lighting changes.
*   **Real-time Performance:** Depth estimation and TSDF integration can be computationally intensive. Performance depends on CPU/GPU capabilities.

## Future Work (Possible Enhancements)

*   **SLAM Frontend Enhancements:**
    *   Implement Local Bundle Adjustment over recent keyframes to refine poses and map points.
    *   Develop robust triangulation of new map points as new keyframes are added, and expand the local map used for PnP.
    *   Integrate loop detection and pose graph optimization for long-term drift correction.
    *   Improve local map management for `solvePnP` to use a wider and more relevant window of map points.
*   **Depth and Reconstruction:**
    *   More sophisticated depth map scaling or fusion techniques.
    *   Investigate volumetric fusion methods beyond TSDF if different trade-offs are needed.
*   **General:**
    *   Integration with IMU data for improved robustness and scale estimation.
    *   UI for easier parameter tuning and control.
